# –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
import tkinter as tk  # –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
from tkinter import filedialog, messagebox, ttk  # –î–∏–∞–ª–æ–≥–æ–≤—ã–µ –æ–∫–Ω–∞, —Å–æ–æ–±—â–µ–Ω–∏—è, –≤–∏–¥–∂–µ—Ç—ã
import threading  # –î–ª—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–π —Ä–∞–±–æ—Ç—ã
import os  # –î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π
import pandas as pd  # –î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–∞–±–ª–∏—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
import re  # –î–ª—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
from collections import Counter  # –î–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —á–∞—Å—Ç–æ—Ç

# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
EXCLUDED_WORDS = {
    # –ü—Ä–µ–¥–ª–æ–≥–∏
    "–≤", "–Ω–∞", "—Å", "–ø–æ", "–∏–∑", "—É", "–∫", "–æ—Ç", "–¥–æ", "–∑–∞", "–æ", "–æ–±", "—Å–æ", "–∏–∑–æ",
    # –°–æ—é–∑—ã
    "–∏", "–∞", "–Ω–æ", "–∏–ª–∏", "–ª–∏–±–æ", "—á—Ç–æ", "—á—Ç–æ–±—ã", "–∫–∞–∫", "–ø–æ—Ç–æ–º—É", "—Ç–∞–∫–∂–µ",
    # –ì–û–°–¢—ã (—à–∞–±–ª–æ–Ω—ã)
    "–≥–æ—Å—Ç", "—Ä", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç", "iso", "—Ç—É", "–æ—Å—Ç", "—Å–Ω–∏–ø", "—Å–ø", "–≥–Ω", "—Ä–¥", "—Å–∞–Ω–ø–∏–Ω"
}

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
try:
    import spacy
    from spacy import displacy # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ä–µ–≤—å–µ–≤

    nlp = spacy.load("ru_core_news_sm")
except Exception:
    raise SystemExit(
        "‚ö†Ô∏è –ú–æ–¥–µ–ª—å spaCy 'ru_core_news_sm' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∫–æ–º–∞–Ω–¥–æ–π:\n"
        "    pip install -U spacy && python -m spacy download ru_core_news_sm"
    )


# ---------------------- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò --------------------------- #

# –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
def clean_links(text: str) -> str:
    text = str(text)
    text = re.sub(r"\(?\b(?:https?://|www\.)\S+\b\)?", "", text) # –£–¥–∞–ª–µ–Ω–∏–µ URL
    text = re.sub(r"=HYPERLINK\(\"[^\"]+\",\"([^\"]+)\"\)", r"\1", text) # –£–¥–∞–ª–µ–Ω–∏–µ –≥–∏–ø–µ—Ä—Å—Å—ã–ª–æ–∫ Excel
    return re.sub(r"\(\s*\)", "", text).strip() # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å–∫–æ–±–æ–∫


# –£–¥–∞–ª–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Å–∫–æ–±–∫–∞—Ö
def remove_brackets(text: str) -> str:
    text = str(text)
    text = re.sub(r"\([^)]*\)", "", text) # –£–¥–∞–ª–µ–Ω–∏–µ –∫—Ä—É–≥–ª—ã—Ö —Å–∫–æ–±–æ–∫
    text = re.sub(r"\[[^\]]*\]", "", text) # –ö–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–æ–∫
    text = re.sub(r"[\(\)\[\]]", "", text) # –û—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å–∫–æ–±–æ–∫
    return re.sub(r"\s+", " ", text).strip() # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤


# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–∫—Å—Ç–∞: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ (–±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏) –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤
def basic_stats(text: str):
    doc = nlp(text)
    tokens = [t for t in doc if not t.is_punct and not t.is_space]
    return len(tokens), len(text)


# –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ n-–≥—Ä–∞–º–º—ã (–±–∏–≥—Ä–∞–º–º—ã/—Ç—Ä–∏–≥—Ä–∞–º–º—ã)
def extract_ngrams(texts, n=2, top_k=3):
    counts = Counter()
    examples = {}  # –ë—É–¥–µ–º —Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–π n-–≥—Ä–∞–º–º—ã

    for txt in texts:
        doc = nlp(txt)
        # –ü–æ–ª—É—á–∞–µ–º –ª–µ–º–º—ã –∏ –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        lemmas = [t.lemma_.lower() for t in doc if t.is_alpha and t.lemma_.lower() not in EXCLUDED_WORDS]
        tokens = [t.text.lower() for t in doc if t.is_alpha and t.lemma_.lower() not in EXCLUDED_WORDS]

        # –°–æ–±–∏—Ä–∞–µ–º n-–≥—Ä–∞–º–º—ã
        for i in range(len(lemmas) - n + 1):
            ngram_lemmas = tuple(lemmas[i:i + n])
            ngram_text = tuple(tokens[i:i + n])

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –≤ n-–≥—Ä–∞–º–º–µ –Ω–µ –≤—Ö–æ–¥–∏—Ç –≤ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ
            if not any(word in EXCLUDED_WORDS for word in ngram_lemmas):
                counts[ngram_lemmas] += 1
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä
                if ngram_lemmas not in examples:
                    examples[ngram_lemmas] = ' '.join(ngram_text)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º n-–≥—Ä–∞–º–º—ã –≤ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ —Å —á–∞—Å—Ç–æ—Ç–∞–º–∏ –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–µ
    return [(gram, cnt, examples[gram]) for gram, cnt in counts.most_common(top_k)]


# –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True,
# –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ ‚â•3 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ –∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –≥–ª–∞–≥–æ–ª
def is_real_sentence(text: str) -> bool:
    doc = nlp(text)
    tokens = [t for t in doc if not t.is_punct and not t.is_space]
    return len(tokens) >= 3 and any(t.pos_ == "VERB" for t in tokens)


# –°–æ–∑–¥–∞–µ—Ç —Å–∏–≥–Ω–∞—Ç—É—Ä—É ROOT + –º–µ—Ç–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –µ–≥–æ –ø—Ä—è–º—ã—Ö –ø–æ—Ç–æ–º–∫–æ–≤,
# —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–µ –ø–æ –ø–æ—Ä—è–¥–∫—É –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
def clause_dep_signature(root_token):
    sig = [root_token.dep_] # –ù–∞—á–∏–Ω–∞–µ–º —Å –∫–æ—Ä–Ω–µ–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞

    # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø—Ä—è–º—ã—Ö –ø–æ—Ç–æ–º–∫–æ–≤ (children), –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    children = [tok for tok in root_token.children
                if not tok.is_punct and not tok.is_space]
    children.sort(key=lambda t: t.i)

    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–∏–ø—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    excluded_deps = {"det", "case", "punct"}
    for child in children:
        if child.dep_ not in excluded_deps:
            sig.append(child.dep_)
    return tuple(sig)


# –£–¥–∞–ª–µ–Ω–∏–µ –ì–û–°–¢–æ–≤
def remove_gost_phrases(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ç–∏–ø–∞ –ì–û–°–¢ –† 57528 ‚Äì 2016 –∏ —Å–ª–æ–≤–æ –ì–û–°–¢."""
    pattern = r"\b–≥–æ—Å—Ç(?:\s*[–∞-—èa-zA-Z]*\s*\d{4,6}(?:\s*[‚Äì-]\s*\d{2,4})?)?\b"
    return re.sub(pattern, "", text, flags=re.IGNORECASE).strip()


# –¢–æ–ø —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
def top_syntactic_structures(sentences, top_n: int = 3):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –Ω–∞—Ö–æ–¥–∏—Ç top_n —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
    –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —á–∞—Å—Ç–æ—Ç—É –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏"""
    counter: Counter[tuple[str, ...]] = Counter()
    examples: dict[tuple[str, ...], tuple[str, "spacy.tokens.Doc"]] = {}

    for sent in sentences:
        sent = remove_gost_phrases(sent)
        doc = nlp(sent)
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ—Ä–Ω–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        for root in (t for t in doc if t.dep_ == "ROOT" and not t.is_punct):
            sig = clause_dep_signature(root)
            if not sig or len(sig) < 2: # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                continue
            counter[sig] += 1
            examples.setdefault(sig, (sent, doc))  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä

    return [(sig, freq, *examples[sig]) for sig, freq in counter.most_common(top_n)]


# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ç–µ–∂–∞ —Å —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –≤ —Å—Ç—Ä–æ–∫—É
def sig_to_str(sig: tuple[str, ...]) -> str:
    return "+".join(sig)


# –ù–∞—Ö–æ–¥–∏—Ç –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–µ —É–∫–∞–∑–∞–Ω–Ω—É—é n-–≥—Ä–∞–º–º—É (–ø–æ –ª–µ–º–º–∞–º —Å–ª–æ–≤)
def find_sentence_with_ngram(sentences, ngram_lemmas):
    n = len(ngram_lemmas)
    for sent in sentences:
        sent = remove_gost_phrases(sent)
        # –ò—â–µ–º –ø–æ –ª–µ–º–º–∞–º, –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–µ
        lemmas = [t.lemma_.lower() for t in nlp(sent) if t.is_alpha and t.lemma_.lower() not in EXCLUDED_WORDS]
        for i in range(len(lemmas) - n + 1):
            if tuple(lemmas[i:i + n]) == ngram_lemmas:
                return sent
    return None


# ---------------------- GUI ------------------------------------------------ #

# –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
class NLPApp(tk.Tk):
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –æ–∫–Ω–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏ –µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    def __init__(self):
        super().__init__()
        self.title("–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (spaCy)")
        self.geometry("1100x800")
        self.resizable(False, False)

        # —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self.file_path = None # –ü—É—Ç—å –∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        self.cleaned_df = None # –û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.report_txt = "" # –¢–µ–∫—Å—Ç –æ—Ç—á–µ—Ç–∞
        self.dep_htmls = [] # –°–ø–∏—Å–æ–∫ HTML-–¥–µ—Ä–µ–≤—å–µ–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

        self._build_ui() # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞

    # –°–æ–∑–¥–∞–µ—Ç –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
    def _build_ui(self):
        top = tk.Frame(self)
        top.pack(fill=tk.X, padx=10, pady=10)

        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        self.btn_load = tk.Button(top, text="üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å Excel", command=self.load_file)
        self.btn_load.pack(side=tk.LEFT, padx=5)

        self.btn_run = tk.Button(top, text="üîç –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑", state=tk.DISABLED,
                                 command=lambda: threading.Thread(target=self.analyze, daemon=True).start())
        self.btn_run.pack(side=tk.LEFT, padx=5)

        self.btn_clear = tk.Button(top, text="üßπ –û—á–∏—Å—Ç–∏—Ç—å", command=self.clear_output)
        self.btn_clear.pack(side=tk.LEFT, padx=5)

        self.btn_save_clean = tk.Button(top, text="üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—á–∏—â–µ–Ω–Ω—ã–µ", state=tk.DISABLED, command=self.save_cleaned)
        self.btn_save_clean.pack(side=tk.LEFT, padx=5)

        self.btn_save_report = tk.Button(top, text="üìù –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç", state=tk.DISABLED, command=self.save_report)
        self.btn_save_report.pack(side=tk.LEFT, padx=5)

        self.btn_save_trees = tk.Button(top, text="üå≥ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ—Ä–µ–≤—å—è", state=tk.DISABLED, command=self.save_trees)
        self.btn_save_trees.pack(side=tk.LEFT, padx=5)

        self.progress = ttk.Progressbar(top, orient="horizontal", length=240, mode="determinate")
        self.progress.pack(side=tk.RIGHT, padx=5)

        self.out = tk.Text(self, wrap=tk.WORD, state=tk.DISABLED)
        self.out.pack(expand=True, fill=tk.BOTH, padx=10, pady=(0, 10))

    # –í—ã–≤–æ–¥–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
    def log(self, msg):
        self.out.config(state=tk.NORMAL)
        self.out.insert(tk.END, msg)
        self.out.see(tk.END)
        self.out.config(state=tk.DISABLED)

    # –û–±–Ω–æ–≤–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
    def set_progress(self, val):
        self.progress["value"] = val
        self.update_idletasks()

    # –û—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–∏–∞–ª–æ–≥ –≤—ã–±–æ—Ä–∞ Excel-—Ñ–∞–π–ª–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    def load_file(self):
        path = filedialog.askopenfilename(title="–í—ã–±–µ—Ä–∏—Ç–µ Excel-—Ñ–∞–π–ª", filetypes=[("Excel", "*.xlsx *.xls")])
        if path:
            self.file_path = path
            self.btn_run.config(state=tk.NORMAL)
            self.log(f"–§–∞–π–ª –≤—ã–±—Ä–∞–Ω: {os.path.basename(path)}\n")

    # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞
    def analyze(self):
        if not self.file_path:
            return
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–µ—Ä–≤–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            self.set_progress(0)
            self.log("–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞...\n")
            df = pd.read_excel(self.file_path).dropna(how="all")
            df = df.replace(['-', '‚Äî', '‚Äì'], pd.NA).dropna(subset=["–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä—É—Å)"])

            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            self.log("–û—á–∏—Å—Ç–∫–∞...\n")
            df["–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä—É—Å)"] = (df["–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä—É—Å)"]
                                    .apply(clean_links)
                                    .apply(remove_brackets)
                                    .str.replace('\n', ' ', regex=False))
            df = df[df["–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä—É—Å)"].str.strip().ne('')]
            df = df[df["–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä—É—Å)"].apply(is_real_sentence)]
            self.set_progress(15)

            # C—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞
            self.log("–†–∞—Å—á—ë—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\n")
            df["num_words"], df["num_chars"] = zip(*df["–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä—É—Å)"].map(basic_stats))
            rows = len(df)
            w_sum, c_sum = df["num_words"].sum(), df["num_chars"].sum()
            w_avg, c_avg = df["num_words"].mean(), df["num_chars"].mean()
            self.set_progress(30)

            # –¢–æ–ø-3 –±–∏–≥—Ä–∞–º–º –∏ —Ç—Ä–∏–≥—Ä–∞–º–º
            self.log("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ n-–≥—Ä–∞–º–º...\n")
            top_bigrams = extract_ngrams(df["–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä—É—Å)"], 2, 3)
            top_trigrams = extract_ngrams(df["–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä—É—Å)"], 3, 3)
            self.set_progress(50)

            # –ê–Ω–∞–ª–∏–∑ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
            self.log("–ü–æ–∏—Å–∫ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä...\n")
            sentences = df["–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä—É—Å)"].tolist()
            top_structs = top_syntactic_structures(sentences, 3)
            self.set_progress(65)

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á–µ—Ç–∞
            self.dep_htmls.clear() # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
            rep = [
                f"–§–∞–π–ª: {os.path.basename(self.file_path)}",
                f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {rows}",
                f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {w_sum}",
                f"–í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {c_sum}",
                f"–°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ —Å–ª–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ: {w_avg:.0f}",
                f"–°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ: {c_avg:.0f}",
            ]

            # –†–∞–∑–¥–µ–ª –æ—Ç—á–µ—Ç–∞ –ø–æ –±–∏–≥—Ä–∞–º–º–∞–º
            rep.append("\n–¢–æ–ø –±–∏–≥—Ä–∞–º–º:")
            for idx, (bg, cnt, example) in enumerate(top_bigrams, 1):
                rep.append(f"  {idx}. {' '.join(bg)} ‚Äî {cnt} (–ø—Ä–∏–º–µ—Ä: {example})")
                # –ü–æ–∏—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —ç—Ç–æ–π –±–∏–≥—Ä–∞–º–º–æ–π
                sent = find_sentence_with_ngram(sentences, bg)
                if sent:
                    doc = nlp(sent)
                    label = f"bigram_{idx}"
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
                    self.dep_htmls.append((label, displacy.render(doc, style="dep", page=True)))
                    rep.append(f"     ‚ñ∂ –ø–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä: {sent}")

            # –†–∞–∑–¥–µ–ª –æ—Ç—á–µ—Ç–∞ –ø–æ —Ç—Ä–∏–≥—Ä–∞–º–º–∞–º
            rep.append("\n–¢–æ–ø —Ç—Ä–∏–≥—Ä–∞–º–º:")
            for idx, (tg, cnt, example) in enumerate(top_trigrams, 1):
                rep.append(f"  {idx}. {' '.join(tg)} ‚Äî {cnt} (–ø—Ä–∏–º–µ—Ä: {example})")
                sent = find_sentence_with_ngram(sentences, tg)
                if sent:
                    doc = nlp(sent)
                    label = f"trigram_{idx}"
                    self.dep_htmls.append((label, displacy.render(doc, style="dep", page=True)))
                    rep.append(f"     ‚ñ∂ –ø–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä: {sent}")

            # –†–∞–∑–¥–µ–ª –æ—Ç—á–µ—Ç–∞ –ø–æ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º
            if top_structs:
                rep.append("\n–¢–æ–ø —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä:")
                for idx, (struct, freq, sent, doc) in enumerate(top_structs, 1):
                    rep.append(f"\n#{idx} (–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è {freq}√ó): {sig_to_str(struct)}")
                    rep.append(sent)
                    rep.append("–ü–æ–¥—Ä–æ–±–Ω—ã–π —Ä–∞–∑–±–æ—Ä:")
                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–±–æ—Ä–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
                    for tok in doc:
                        if tok.is_punct:
                            continue
                        rep.append(f"{tok.text:<15} {tok.pos_:<5} {tok.dep_:<10} ‚Üí {tok.head.text}")

                    label = f"struct_{idx}"
                    self.dep_htmls.append((label, displacy.render(doc, style="dep", page=True)))
            else:
                rep.append("\n–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.report_txt = "\n".join(rep)
            self.log("\n–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω.\n")
            self.log(self.report_txt + "\n")

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
            self.cleaned_df = df.drop(columns=["num_words", "num_chars"], errors="ignore")
            # –ê–∫—Ç–∏–≤–∞—Ü–∏—è –∫–Ω–æ–ø–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            self.btn_save_clean.config(state=tk.NORMAL)
            self.btn_save_report.config(state=tk.NORMAL)
            if self.dep_htmls:
                self.btn_save_trees.config(state=tk.NORMAL)
            self.set_progress(100) # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

        except Exception as exc:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Å –≤—ã–≤–æ–¥–æ–º —Å–æ–æ–±—â–µ–Ω–∏—è
            messagebox.showerror("–û—à–∏–±–∫–∞", str(exc))
            self.log(f"–û—à–∏–±–∫–∞: {exc}\n")
            self.set_progress(0)

    # –û—á–∏—â–∞–µ—Ç –≤—Å–µ –ø–æ–ª—è –≤—ã–≤–æ–¥–∞ –∏ —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    def clear_output(self):
        self.out.config(state=tk.NORMAL)
        self.out.delete("1.0", tk.END)
        self.out.config(state=tk.DISABLED)
        self.set_progress(0)
        self.cleaned_df = None
        self.report_txt = ""
        self.dep_htmls = []
        for btn in (self.btn_save_clean, self.btn_save_report, self.btn_save_trees):
            btn.config(state=tk.DISABLED)

    # –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤—ã–π Excel-—Ñ–∞–π–ª
    def save_cleaned(self):
        if self.cleaned_df is None:
            return
        path = filedialog.asksaveasfilename(title="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", defaultextension=".xlsx",
                                            filetypes=[("Excel", "*.xlsx")])
        if path:
            self.cleaned_df.to_excel(path, index=False)
            messagebox.showinfo("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ", f"–û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ‚Üí {path}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
    def save_report(self):
        if not self.report_txt:
            return
        path = filedialog.asksaveasfilename(title="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç", defaultextension=".txt",
                                            filetypes=[("Text", "*.txt")])
        if path:
            with open(path, "w", encoding="utf-8") as f:
                f.write(self.report_txt)
            messagebox.showinfo("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ", f"–û—Ç—á—ë—Ç ‚Üí {path}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ä–µ–≤—å–µ–≤ –≤ HTML-—Ñ–∞–π–ª—ã
    def save_trees(self):
        if not self.dep_htmls:
            return
        dir_path = filedialog.askdirectory(title="–í—ã–±–µ—Ä–∏—Ç–µ –ø–∞–ø–∫—É –¥–ª—è HTML-–¥–µ—Ä–µ–≤—å–µ–≤")
        if not dir_path:
            return
        for label, html in self.dep_htmls:
            file_path = os.path.join(dir_path, f"{label}.html")
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(html)
        messagebox.showinfo("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ", f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.dep_htmls)} HTML-—Ñ–∞–π–ª–æ–≤ –≤ {dir_path}")


if __name__ == "__main__":
    NLPApp().mainloop()